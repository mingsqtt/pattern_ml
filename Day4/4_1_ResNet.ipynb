{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\apps\\Anaconda3\\envs\\keras-cv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\apps\\Anaconda3\\envs\\keras-cv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\apps\\Anaconda3\\envs\\keras-cv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\apps\\Anaconda3\\envs\\keras-cv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\apps\\Anaconda3\\envs\\keras-cv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\apps\\Anaconda3\\envs\\keras-cv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,CSVLogger,LearningRateScheduler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implt(img):\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "                            # Set up 'ggplot' style\n",
    "plt.style.use('ggplot')     # if want to use the default style, set 'classic'\n",
    "plt.rcParams['ytick.right']     = True\n",
    "plt.rcParams['ytick.labelright']= True\n",
    "plt.rcParams['ytick.left']      = False\n",
    "plt.rcParams['ytick.labelleft'] = False\n",
    "plt.rcParams['font.family']     = 'Arial'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"D:/projects/pattern_ml/Day4\")\n",
    "\n",
    "data            = cifar10.load_data()\n",
    "(trDat, trLbl)  = data[0]\n",
    "(tsDat, tsLbl)  = data[1]\n",
    "\n",
    "\n",
    "\n",
    "                            # Convert the data into 'float32'\n",
    "                            # Rescale the values from 0~255 to 0~1\n",
    "trDat       = trDat.astype('float32')/255\n",
    "tsDat       = tsDat.astype('float32')/255\n",
    "\n",
    "\n",
    "                            # Retrieve the row size of each image\n",
    "                            # Retrieve the column size of each image\n",
    "imgrows     = trDat.shape[1]\n",
    "imgclms     = trDat.shape[2]\n",
    "channel     = trDat.shape[3]\n",
    "\n",
    "\n",
    "                            # Perform one hot encoding on the labels\n",
    "                            # Retrieve the number of classes in this problem\n",
    "trLbl       = to_categorical(trLbl)\n",
    "tsLbl       = to_categorical(tsLbl)\n",
    "num_classes = tsLbl.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meanTrDat   = np.mean(trDat,axis=0)\n",
    "#trDat       = trDat-meanTrDat\n",
    "#tsDat       = tsDat-meanTrDat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Inpt_conv (Conv2D)              (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Inpt_bn (BatchNormalization)    (None, 32, 32, 16)   64          Inpt_conv[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Inpt_relu (Activation)          (None, 32, 32, 16)   0           Inpt_bn[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_Res1_conv (Conv2D)    (None, 32, 32, 16)   2320        Inpt_relu[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_Res1_bn (BatchNormali (None, 32, 32, 16)   64          Stg1_Blk1_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_Res1_relu (Activation (None, 32, 32, 16)   0           Stg1_Blk1_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_Res2_conv (Conv2D)    (None, 32, 32, 16)   2320        Stg1_Blk1_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_Res2_bn (BatchNormali (None, 32, 32, 16)   64          Stg1_Blk1_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_add (Add)             (None, 32, 32, 16)   0           Inpt_relu[0][0]                  \n",
      "                                                                 Stg1_Blk1_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_relu (Activation)     (None, 32, 32, 16)   0           Stg1_Blk1_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_Res1_conv (Conv2D)    (None, 32, 32, 16)   2320        Stg1_Blk1_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_Res1_bn (BatchNormali (None, 32, 32, 16)   64          Stg1_Blk2_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_Res1_relu (Activation (None, 32, 32, 16)   0           Stg1_Blk2_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_Res2_conv (Conv2D)    (None, 32, 32, 16)   2320        Stg1_Blk2_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_Res2_bn (BatchNormali (None, 32, 32, 16)   64          Stg1_Blk2_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_add (Add)             (None, 32, 32, 16)   0           Stg1_Blk1_relu[0][0]             \n",
      "                                                                 Stg1_Blk2_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_relu (Activation)     (None, 32, 32, 16)   0           Stg1_Blk2_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_Res1_conv (Conv2D)    (None, 32, 32, 16)   2320        Stg1_Blk2_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_Res1_bn (BatchNormali (None, 32, 32, 16)   64          Stg1_Blk3_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_Res1_relu (Activation (None, 32, 32, 16)   0           Stg1_Blk3_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_Res2_conv (Conv2D)    (None, 32, 32, 16)   2320        Stg1_Blk3_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_Res2_bn (BatchNormali (None, 32, 32, 16)   64          Stg1_Blk3_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_add (Add)             (None, 32, 32, 16)   0           Stg1_Blk2_relu[0][0]             \n",
      "                                                                 Stg1_Blk3_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_relu (Activation)     (None, 32, 32, 16)   0           Stg1_Blk3_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_Res1_conv (Conv2D)    (None, 16, 16, 32)   4640        Stg1_Blk3_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_Res1_bn (BatchNormali (None, 16, 16, 32)   128         Stg2_Blk1_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_Res1_relu (Activation (None, 16, 16, 32)   0           Stg2_Blk1_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_Res2_conv (Conv2D)    (None, 16, 16, 32)   9248        Stg2_Blk1_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_lin_conv (Conv2D)     (None, 16, 16, 32)   544         Stg1_Blk3_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_Res2_bn (BatchNormali (None, 16, 16, 32)   128         Stg2_Blk1_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_add (Add)             (None, 16, 16, 32)   0           Stg2_Blk1_lin_conv[0][0]         \n",
      "                                                                 Stg2_Blk1_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_relu (Activation)     (None, 16, 16, 32)   0           Stg2_Blk1_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_Res1_conv (Conv2D)    (None, 16, 16, 32)   9248        Stg2_Blk1_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_Res1_bn (BatchNormali (None, 16, 16, 32)   128         Stg2_Blk2_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_Res1_relu (Activation (None, 16, 16, 32)   0           Stg2_Blk2_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_Res2_conv (Conv2D)    (None, 16, 16, 32)   9248        Stg2_Blk2_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_Res2_bn (BatchNormali (None, 16, 16, 32)   128         Stg2_Blk2_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_add (Add)             (None, 16, 16, 32)   0           Stg2_Blk1_relu[0][0]             \n",
      "                                                                 Stg2_Blk2_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_relu (Activation)     (None, 16, 16, 32)   0           Stg2_Blk2_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_Res1_conv (Conv2D)    (None, 16, 16, 32)   9248        Stg2_Blk2_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_Res1_bn (BatchNormali (None, 16, 16, 32)   128         Stg2_Blk3_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_Res1_relu (Activation (None, 16, 16, 32)   0           Stg2_Blk3_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_Res2_conv (Conv2D)    (None, 16, 16, 32)   9248        Stg2_Blk3_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_Res2_bn (BatchNormali (None, 16, 16, 32)   128         Stg2_Blk3_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_add (Add)             (None, 16, 16, 32)   0           Stg2_Blk2_relu[0][0]             \n",
      "                                                                 Stg2_Blk3_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_relu (Activation)     (None, 16, 16, 32)   0           Stg2_Blk3_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_Res1_conv (Conv2D)    (None, 8, 8, 64)     18496       Stg2_Blk3_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_Res1_bn (BatchNormali (None, 8, 8, 64)     256         Stg3_Blk1_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_Res1_relu (Activation (None, 8, 8, 64)     0           Stg3_Blk1_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_Res2_conv (Conv2D)    (None, 8, 8, 64)     36928       Stg3_Blk1_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_lin_conv (Conv2D)     (None, 8, 8, 64)     2112        Stg2_Blk3_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_Res2_bn (BatchNormali (None, 8, 8, 64)     256         Stg3_Blk1_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_add (Add)             (None, 8, 8, 64)     0           Stg3_Blk1_lin_conv[0][0]         \n",
      "                                                                 Stg3_Blk1_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_relu (Activation)     (None, 8, 8, 64)     0           Stg3_Blk1_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_Res1_conv (Conv2D)    (None, 8, 8, 64)     36928       Stg3_Blk1_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_Res1_bn (BatchNormali (None, 8, 8, 64)     256         Stg3_Blk2_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_Res1_relu (Activation (None, 8, 8, 64)     0           Stg3_Blk2_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_Res2_conv (Conv2D)    (None, 8, 8, 64)     36928       Stg3_Blk2_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_Res2_bn (BatchNormali (None, 8, 8, 64)     256         Stg3_Blk2_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_add (Add)             (None, 8, 8, 64)     0           Stg3_Blk1_relu[0][0]             \n",
      "                                                                 Stg3_Blk2_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_relu (Activation)     (None, 8, 8, 64)     0           Stg3_Blk2_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_Res1_conv (Conv2D)    (None, 8, 8, 64)     36928       Stg3_Blk2_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_Res1_bn (BatchNormali (None, 8, 8, 64)     256         Stg3_Blk3_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_Res1_relu (Activation (None, 8, 8, 64)     0           Stg3_Blk3_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_Res2_conv (Conv2D)    (None, 8, 8, 64)     36928       Stg3_Blk3_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_Res2_bn (BatchNormali (None, 8, 8, 64)     256         Stg3_Blk3_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_add (Add)             (None, 8, 8, 64)     0           Stg3_Blk2_relu[0][0]             \n",
      "                                                                 Stg3_Blk3_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_relu (Activation)     (None, 8, 8, 64)     0           Stg3_Blk3_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "AvgPool (AveragePooling2D)      (None, 1, 1, 64)     0           Stg3_Blk3_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 64)           0           AvgPool[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           650         flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seed        = 29\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "optmz       = optimizers.Adam(lr=0.001)\n",
    "modelname   = 'cifar10ResV1Cfg5'\n",
    "\n",
    "                           \n",
    "    \n",
    "\n",
    "def resLyr(inputs,\n",
    "           numFilters=16,\n",
    "           kernelSz=3,\n",
    "           strides=1,\n",
    "           activation='relu',\n",
    "           batchNorm=True,\n",
    "           convFirst=True,\n",
    "           lyrName=None):\n",
    "\n",
    "    convLyr     = Conv2D(numFilters,\n",
    "                         kernel_size=kernelSz,\n",
    "                         strides=strides,\n",
    "                         padding='same',\n",
    "                         kernel_initializer='he_normal',\n",
    "                         kernel_regularizer=l2(1e-4),\n",
    "                         name=lyrName+'_conv' if lyrName else None)\n",
    "    x           = inputs\n",
    "    \n",
    "    if convFirst:\n",
    "        x       = convLyr(x)\n",
    "        \n",
    "        if batchNorm:\n",
    "            x   = BatchNormalization(name=lyrName+'_bn' if lyrName else None)(x)\n",
    "            \n",
    "        if activation is not None:\n",
    "            x   = Activation(activation,\n",
    "                             name=lyrName+'_'+activation if lyrName else None)(x)\n",
    "    else:\n",
    "        if batchNorm:\n",
    "            x   = BatchNormalization(name=lyrName+'_bn' if lyrName else None)(x)\n",
    "            \n",
    "        if activation is not None:\n",
    "            x   = Activation(activation,\n",
    "                             name=lyrName+'_'+activation if lyrName else None)(x)\n",
    "            \n",
    "        x       = convLyr(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resBlkV1(inputs,\n",
    "             numFilters=16,\n",
    "             numBlocks=3,\n",
    "             downsampleOnFirst=True,\n",
    "             names=None):\n",
    "    \n",
    "    x       = inputs\n",
    "    \n",
    "    for run in range(0,numBlocks):\n",
    "        strides = 1\n",
    "        blkStr  = str(run+1)\n",
    "        \n",
    "        if downsampleOnFirst and run == 0:\n",
    "            strides     = 2\n",
    "            \n",
    "            \n",
    "        y       = resLyr(inputs=x,\n",
    "                         numFilters=numFilters,\n",
    "                         strides=strides,\n",
    "                         lyrName=names+'_Blk'+blkStr+'_Res1' if names else None)\n",
    "        y       = resLyr(inputs=y,\n",
    "                         numFilters=numFilters,\n",
    "                         activation=None,\n",
    "                         lyrName=names+'_Blk'+blkStr+'_Res2' if names else None)\n",
    "        \n",
    "        if downsampleOnFirst and run == 0:\n",
    "            x   = resLyr(inputs=x,\n",
    "                         numFilters=numFilters,\n",
    "                         kernelSz=1,\n",
    "                         strides=strides,\n",
    "                         activation=None,\n",
    "                         batchNorm=False,\n",
    "                         lyrName=names+'_Blk'+blkStr+'_lin' if names else None)\n",
    "\n",
    "        x       = add([x,y],\n",
    "                      name=names+'_Blk'+blkStr+'_add' if names else None)\n",
    "        x       = Activation('relu',\n",
    "                             name=names+'_Blk'+blkStr+'_relu' if names else None)(x)\n",
    "        \n",
    "    return x\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def createResNetV1(inputShape=(32,32,3),\n",
    "                   numClasses=10):\n",
    "    \n",
    "    inputs      = Input(shape=inputShape)\n",
    "    v           = resLyr(inputs,\n",
    "                         lyrName='Inpt')\n",
    "    v           = resBlkV1(inputs=v,\n",
    "                           numFilters=16,\n",
    "                           numBlocks=3,\n",
    "                           downsampleOnFirst=False,\n",
    "                           names='Stg1')\n",
    "    v           = resBlkV1(inputs=v,\n",
    "                           numFilters=32,\n",
    "                           numBlocks=3,\n",
    "                           downsampleOnFirst=True,\n",
    "                           names='Stg2')\n",
    "    v           = resBlkV1(inputs=v,\n",
    "                           numFilters=64,\n",
    "                           numBlocks=3,\n",
    "                           downsampleOnFirst=True,\n",
    "                           names='Stg3')\n",
    "    v           = AveragePooling2D(pool_size=8,\n",
    "                                   name='AvgPool')(v)\n",
    "    v           = Flatten()(v)\n",
    "    outputs     = Dense(numClasses,\n",
    "                        activation='softmax',\n",
    "                        kernel_initializer='he_normal')(v)\n",
    "    model       = Model(inputs=inputs,outputs=outputs)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=optmz, \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                # Setup the models\n",
    "model       = createResNetV1()  # This is meant for training\n",
    "modelGo     = createResNetV1()  # This is used for final testing\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrSchedule(epoch):\n",
    "    lr  = 1e-3\n",
    "    \n",
    "    if epoch > 160:\n",
    "        lr  *= 0.5e-3\n",
    "        \n",
    "    elif epoch > 140:\n",
    "        lr  *= 1e-3\n",
    "        \n",
    "    elif epoch > 120:\n",
    "        lr  *= 1e-2\n",
    "        \n",
    "    elif epoch > 80:\n",
    "        lr  *= 1e-1\n",
    "        \n",
    "    print('Learning rate: ', lr)\n",
    "    \n",
    "    return lr\n",
    "\n",
    "LRScheduler     = LearningRateScheduler(lrSchedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "                            # Create checkpoint for the training\n",
    "                            # This checkpoint performs model saving when\n",
    "                            # an epoch gives highest testing accuracy\n",
    "filepath        = modelname + \".hdf5\"\n",
    "checkpoint      = ModelCheckpoint(filepath, \n",
    "                                  monitor='val_acc', \n",
    "                                  verbose=0, \n",
    "                                  save_best_only=True, \n",
    "                                  mode='max')\n",
    "\n",
    "                            # Log the epoch detail into csv\n",
    "csv_logger      = CSVLogger(modelname +'.csv')\n",
    "callbacks_list  = [checkpoint,csv_logger,LRScheduler]\n",
    "#callbacks_list  = [checkpoint,csv_logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epoch 1/200\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 1.5949 - acc: 0.4798 - val_loss: 1.4628 - val_acc: 0.5450\n",
      "Learning rate:  0.001\n",
      "Epoch 2/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 1.2569 - acc: 0.6069 - val_loss: 1.1207 - val_acc: 0.6511\n",
      "Learning rate:  0.001\n",
      "Epoch 3/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 1.1077 - acc: 0.6635 - val_loss: 1.5279 - val_acc: 0.5769\n",
      "Learning rate:  0.001\n",
      "Epoch 4/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 1.0111 - acc: 0.7053 - val_loss: 1.6284 - val_acc: 0.5956\n",
      "Learning rate:  0.001\n",
      "Epoch 5/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.9455 - acc: 0.7289 - val_loss: 1.0536 - val_acc: 0.7054\n",
      "Learning rate:  0.001\n",
      "Epoch 6/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.8887 - acc: 0.7511 - val_loss: 1.0283 - val_acc: 0.7073\n",
      "Learning rate:  0.001\n",
      "Epoch 7/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.8596 - acc: 0.7617 - val_loss: 1.1484 - val_acc: 0.7013\n",
      "Learning rate:  0.001\n",
      "Epoch 8/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.8250 - acc: 0.7766 - val_loss: 1.0642 - val_acc: 0.7130\n",
      "Learning rate:  0.001\n",
      "Epoch 9/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.8008 - acc: 0.7857 - val_loss: 1.0426 - val_acc: 0.7173\n",
      "Learning rate:  0.001\n",
      "Epoch 10/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.7862 - acc: 0.7888 - val_loss: 0.9190 - val_acc: 0.7528\n",
      "Learning rate:  0.001\n",
      "Epoch 11/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.7667 - acc: 0.7972 - val_loss: 0.9111 - val_acc: 0.7534\n",
      "Learning rate:  0.001\n",
      "Epoch 12/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.7527 - acc: 0.8038 - val_loss: 0.7955 - val_acc: 0.7956\n",
      "Learning rate:  0.001\n",
      "Epoch 13/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.7420 - acc: 0.8068 - val_loss: 0.8410 - val_acc: 0.7830\n",
      "Learning rate:  0.001\n",
      "Epoch 14/200\n",
      "1563/1562 [==============================] - 26s 17ms/step - loss: 0.7177 - acc: 0.8165 - val_loss: 0.8591 - val_acc: 0.7805\n",
      "Learning rate:  0.001\n",
      "Epoch 15/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.7184 - acc: 0.8176 - val_loss: 0.7827 - val_acc: 0.7997\n",
      "Learning rate:  0.001\n",
      "Epoch 16/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.7024 - acc: 0.8251 - val_loss: 1.3253 - val_acc: 0.6689\n",
      "Learning rate:  0.001\n",
      "Epoch 17/200\n",
      "1563/1562 [==============================] - 26s 17ms/step - loss: 0.6941 - acc: 0.8268 - val_loss: 0.9156 - val_acc: 0.7717\n",
      "Learning rate:  0.001\n",
      "Epoch 18/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.6873 - acc: 0.8306 - val_loss: 1.1159 - val_acc: 0.7255\n",
      "Learning rate:  0.001\n",
      "Epoch 19/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.6802 - acc: 0.8310 - val_loss: 0.8738 - val_acc: 0.7688- loss: 0.6803 - acc\n",
      "Learning rate:  0.001\n",
      "Epoch 20/200\n",
      "1563/1562 [==============================] - 26s 17ms/step - loss: 0.6733 - acc: 0.8345 - val_loss: 1.0061 - val_acc: 0.7538\n",
      "Learning rate:  0.001\n",
      "Epoch 21/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.6666 - acc: 0.8374 - val_loss: 1.0619 - val_acc: 0.7425ss: 0.6663 - acc:\n",
      "Learning rate:  0.001\n",
      "Epoch 22/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.6608 - acc: 0.8403 - val_loss: 1.1171 - val_acc: 0.7349\n",
      "Learning rate:  0.001\n",
      "Epoch 23/200\n",
      "1563/1562 [==============================] - 26s 17ms/step - loss: 0.6580 - acc: 0.8416 - val_loss: 0.9062 - val_acc: 0.7776\n",
      "Learning rate:  0.001\n",
      "Epoch 24/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.6547 - acc: 0.8425 - val_loss: 0.7859 - val_acc: 0.7975\n",
      "Learning rate:  0.001\n",
      "Epoch 25/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.6505 - acc: 0.8442 - val_loss: 0.8198 - val_acc: 0.8028\n",
      "Learning rate:  0.001\n",
      "Epoch 26/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.6413 - acc: 0.8480 - val_loss: 0.7317 - val_acc: 0.8219\n",
      "Learning rate:  0.001\n",
      "Epoch 27/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.6343 - acc: 0.8503 - val_loss: 0.8102 - val_acc: 0.7949\n",
      "Learning rate:  0.001\n",
      "Epoch 28/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.6366 - acc: 0.8495 - val_loss: 0.7529 - val_acc: 0.8258\n",
      "Learning rate:  0.001\n",
      "Epoch 29/200\n",
      "1563/1562 [==============================] - 26s 17ms/step - loss: 0.6348 - acc: 0.8513 - val_loss: 0.9176 - val_acc: 0.7848\n",
      "Learning rate:  0.001\n",
      "Epoch 30/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.6305 - acc: 0.8515 - val_loss: 0.7808 - val_acc: 0.8164\n",
      "Learning rate:  0.001\n",
      "Epoch 31/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.6229 - acc: 0.8555 - val_loss: 0.9616 - val_acc: 0.7682\n",
      "Learning rate:  0.001\n",
      "Epoch 32/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.6225 - acc: 0.8529 - val_loss: 0.6654 - val_acc: 0.8425\n",
      "Learning rate:  0.001\n",
      "Epoch 33/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.6235 - acc: 0.8547 - val_loss: 0.7191 - val_acc: 0.8297\n",
      "Learning rate:  0.001\n",
      "Epoch 34/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.6187 - acc: 0.8553 - val_loss: 0.7144 - val_acc: 0.8325\n",
      "Learning rate:  0.001\n",
      "Epoch 35/200\n",
      "1563/1562 [==============================] - 26s 17ms/step - loss: 0.6169 - acc: 0.8576 - val_loss: 0.7269 - val_acc: 0.8266\n",
      "Learning rate:  0.001\n",
      "Epoch 36/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.6119 - acc: 0.8583 - val_loss: 0.7369 - val_acc: 0.8210\n",
      "Learning rate:  0.001\n",
      "Epoch 37/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.6121 - acc: 0.8578 - val_loss: 0.6645 - val_acc: 0.8457\n",
      "Learning rate:  0.001\n",
      "Epoch 38/200\n",
      "1563/1562 [==============================] - 26s 17ms/step - loss: 0.6104 - acc: 0.8596 - val_loss: 0.7760 - val_acc: 0.8160\n",
      "Learning rate:  0.001\n",
      "Epoch 39/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.6075 - acc: 0.8593 - val_loss: 0.7631 - val_acc: 0.8225\n",
      "Learning rate:  0.001\n",
      "Epoch 40/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.6067 - acc: 0.8591 - val_loss: 0.8859 - val_acc: 0.7920\n",
      "Learning rate:  0.001\n",
      "Epoch 41/200\n",
      "1563/1562 [==============================] - 26s 17ms/step - loss: 0.6068 - acc: 0.8626 - val_loss: 0.7498 - val_acc: 0.8266\n",
      "Learning rate:  0.001\n",
      "Epoch 42/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.6030 - acc: 0.8632 - val_loss: 0.6521 - val_acc: 0.8505\n",
      "Learning rate:  0.001\n",
      "Epoch 43/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.6002 - acc: 0.8623 - val_loss: 0.9468 - val_acc: 0.7763\n",
      "Learning rate:  0.001\n",
      "Epoch 44/200\n",
      "1563/1562 [==============================] - 26s 17ms/step - loss: 0.5990 - acc: 0.8630 - val_loss: 0.8252 - val_acc: 0.7983\n",
      "Learning rate:  0.001\n",
      "Epoch 45/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.5994 - acc: 0.8641 - val_loss: 0.7675 - val_acc: 0.8194\n",
      "Learning rate:  0.001\n",
      "Epoch 46/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.5947 - acc: 0.8646 - val_loss: 0.7107 - val_acc: 0.8345\n",
      "Learning rate:  0.001\n",
      "Epoch 47/200\n",
      "1563/1562 [==============================] - 26s 17ms/step - loss: 0.5929 - acc: 0.8647 - val_loss: 0.7595 - val_acc: 0.8243\n",
      "Learning rate:  0.001\n",
      "Epoch 48/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.5934 - acc: 0.8648 - val_loss: 0.7536 - val_acc: 0.8175\n",
      "Learning rate:  0.001\n",
      "Epoch 49/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.5894 - acc: 0.8681 - val_loss: 0.7554 - val_acc: 0.8156\n",
      "Learning rate:  0.001\n",
      "Epoch 50/200\n",
      "1563/1562 [==============================] - 26s 17ms/step - loss: 0.5852 - acc: 0.8684 - val_loss: 1.1438 - val_acc: 0.7485\n",
      "Learning rate:  0.001\n",
      "Epoch 51/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.5843 - acc: 0.8677 - val_loss: 0.7926 - val_acc: 0.8121\n",
      "Learning rate:  0.001\n",
      "Epoch 52/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.5904 - acc: 0.8657 - val_loss: 0.6790 - val_acc: 0.8368\n",
      "Learning rate:  0.001\n",
      "Epoch 53/200\n",
      "1563/1562 [==============================] - 26s 17ms/step - loss: 0.5870 - acc: 0.8674 - val_loss: 0.7929 - val_acc: 0.8161\n",
      "Learning rate:  0.001\n",
      "Epoch 54/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.5836 - acc: 0.8687 - val_loss: 0.7678 - val_acc: 0.8234\n",
      "Learning rate:  0.001\n",
      "Epoch 55/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.5855 - acc: 0.8697 - val_loss: 0.8815 - val_acc: 0.78611s - loss\n",
      "Learning rate:  0.001\n",
      "Epoch 56/200\n",
      "1563/1562 [==============================] - 26s 17ms/step - loss: 0.5826 - acc: 0.8683 - val_loss: 0.7412 - val_acc: 0.8299\n",
      "Learning rate:  0.001\n",
      "Epoch 57/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.5793 - acc: 0.8717 - val_loss: 0.6856 - val_acc: 0.8435\n",
      "Learning rate:  0.001\n",
      "Epoch 58/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.5797 - acc: 0.8712 - val_loss: 0.9922 - val_acc: 0.7674\n",
      "Learning rate:  0.001\n",
      "Epoch 59/200\n",
      "1563/1562 [==============================] - 26s 17ms/step - loss: 0.5761 - acc: 0.8702 - val_loss: 0.8919 - val_acc: 0.7919\n",
      "Learning rate:  0.001\n",
      "Epoch 60/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.5766 - acc: 0.8706 - val_loss: 0.7289 - val_acc: 0.8254\n",
      "Learning rate:  0.001\n",
      "Epoch 61/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.5709 - acc: 0.8743 - val_loss: 0.7055 - val_acc: 0.8287\n",
      "Learning rate:  0.001\n",
      "Epoch 62/200\n",
      "1563/1562 [==============================] - 26s 17ms/step - loss: 0.5763 - acc: 0.8717 - val_loss: 0.7336 - val_acc: 0.8286\n",
      "Learning rate:  0.001\n",
      "Epoch 63/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.5691 - acc: 0.8735 - val_loss: 0.6921 - val_acc: 0.8396\n",
      "Learning rate:  0.001\n",
      "Epoch 64/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.5706 - acc: 0.8728 - val_loss: 0.7513 - val_acc: 0.8216\n",
      "Learning rate:  0.001\n",
      "Epoch 65/200\n",
      "1563/1562 [==============================] - 26s 17ms/step - loss: 0.5709 - acc: 0.8728 - val_loss: 0.7089 - val_acc: 0.8488\n",
      "Learning rate:  0.001\n",
      "Epoch 66/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.5705 - acc: 0.8748 - val_loss: 0.6751 - val_acc: 0.8466\n",
      "Learning rate:  0.001\n",
      "Epoch 67/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.5717 - acc: 0.8722 - val_loss: 0.6825 - val_acc: 0.8421\n",
      "Learning rate:  0.001\n",
      "Epoch 68/200\n",
      "1563/1562 [==============================] - 26s 17ms/step - loss: 0.5651 - acc: 0.8744 - val_loss: 0.6648 - val_acc: 0.8479\n",
      "Learning rate:  0.001\n",
      "Epoch 69/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.5647 - acc: 0.8758 - val_loss: 0.7213 - val_acc: 0.8344\n",
      "Learning rate:  0.001\n",
      "Epoch 70/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.5644 - acc: 0.8747 - val_loss: 0.7565 - val_acc: 0.8276\n",
      "Learning rate:  0.001\n",
      "Epoch 71/200\n",
      "1563/1562 [==============================] - 26s 17ms/step - loss: 0.5650 - acc: 0.8767 - val_loss: 0.8224 - val_acc: 0.8033\n",
      "Learning rate:  0.001\n",
      "Epoch 72/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.5622 - acc: 0.8764 - val_loss: 0.6943 - val_acc: 0.8430\n",
      "Learning rate:  0.001\n",
      "Epoch 73/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.5610 - acc: 0.8761 - val_loss: 0.6272 - val_acc: 0.8579\n",
      "Learning rate:  0.001\n",
      "Epoch 74/200\n",
      "1563/1562 [==============================] - 26s 17ms/step - loss: 0.5624 - acc: 0.8752 - val_loss: 0.6751 - val_acc: 0.8427\n",
      "Learning rate:  0.001\n",
      "Epoch 75/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.5595 - acc: 0.8778 - val_loss: 0.8157 - val_acc: 0.8014\n",
      "Learning rate:  0.001\n",
      "Epoch 76/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.5636 - acc: 0.8739 - val_loss: 0.6030 - val_acc: 0.8652\n",
      "Learning rate:  0.001\n",
      "Epoch 77/200\n",
      "1563/1562 [==============================] - 26s 17ms/step - loss: 0.5547 - acc: 0.8792 - val_loss: 0.6322 - val_acc: 0.8537\n",
      "Learning rate:  0.001\n",
      "Epoch 78/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.5581 - acc: 0.8775 - val_loss: 0.6813 - val_acc: 0.8447\n",
      "Learning rate:  0.001\n",
      "Epoch 79/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.5577 - acc: 0.8760 - val_loss: 0.6650 - val_acc: 0.8471\n",
      "Learning rate:  0.001\n",
      "Epoch 80/200\n",
      "1563/1562 [==============================] - 26s 17ms/step - loss: 0.5562 - acc: 0.8774 - val_loss: 0.8159 - val_acc: 0.8074\n",
      "Learning rate:  0.001\n",
      "Epoch 81/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.5575 - acc: 0.8778 - val_loss: 0.6889 - val_acc: 0.8401\n",
      "Learning rate:  0.0001\n",
      "Epoch 82/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.4743 - acc: 0.9066 - val_loss: 0.5232 - val_acc: 0.8900\n",
      "Learning rate:  0.0001\n",
      "Epoch 83/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.4418 - acc: 0.9157 - val_loss: 0.5139 - val_acc: 0.8949\n",
      "Learning rate:  0.0001\n",
      "Epoch 84/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.4233 - acc: 0.9210 - val_loss: 0.5006 - val_acc: 0.8968\n",
      "Learning rate:  0.0001\n",
      "Epoch 85/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.4172 - acc: 0.9221 - val_loss: 0.5071 - val_acc: 0.8947\n",
      "Learning rate:  0.0001\n",
      "Epoch 86/200\n",
      "1563/1562 [==============================] - 26s 17ms/step - loss: 0.4047 - acc: 0.9257 - val_loss: 0.4870 - val_acc: 0.8994\n",
      "Learning rate:  0.0001\n",
      "Epoch 87/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.3963 - acc: 0.9267 - val_loss: 0.4837 - val_acc: 0.8988\n",
      "Learning rate:  0.0001\n",
      "Epoch 88/200\n",
      "1563/1562 [==============================] - 27s 17ms/step - loss: 0.3910 - acc: 0.9267 - val_loss: 0.4805 - val_acc: 0.9014\n",
      "Learning rate:  0.0001\n",
      "Epoch 89/200\n",
      " 151/1562 [=>............................] - ETA: 23s - loss: 0.3748 - acc: 0.9307"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-ff8e76109ebd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrDat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m                     callbacks=callbacks_list)\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                             \u001b[1;31m# Fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\apps\\Anaconda3\\envs\\keras-cv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2175\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2176\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2177\u001b[1;33m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   2178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2179\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[1;32md:\\apps\\Anaconda3\\envs\\keras-cv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         outs = model.train_on_batch(\n\u001b[1;32m--> 176\u001b[1;33m             x, y, sample_weight=sample_weight, class_weight=class_weight)\n\u001b[0m\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\apps\\Anaconda3\\envs\\keras-cv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1938\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1939\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1940\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1941\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1942\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\apps\\Anaconda3\\envs\\keras-cv\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2984\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 2986\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2988\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\apps\\Anaconda3\\envs\\keras-cv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(width_shift_range=0.1,\n",
    "                             height_shift_range=0.1,\n",
    "                             rotation_range=20,\n",
    "                             horizontal_flip=True,\n",
    "                             vertical_flip=False)\n",
    "\n",
    "model.fit_generator(datagen.flow(trDat, trLbl, batch_size=32),\n",
    "                    validation_data=(tsDat, tsLbl),\n",
    "                    epochs=200, \n",
    "                    verbose=1,\n",
    "                    steps_per_epoch=len(trDat)/32,\n",
    "                    callbacks=callbacks_list)\n",
    "\n",
    "                            # Fit the model\n",
    "                            # This is where the training starts\n",
    "#model.fit(trDat, \n",
    "#          trLbl, \n",
    "#          validation_data=(tsDat, tsLbl), \n",
    "#          epochs=200, \n",
    "#          batch_size=128,\n",
    "#          shuffle=True,\n",
    "#          callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                            # Now the training is complete, we get\n",
    "                            # another object to load the weights\n",
    "                            # compile it, so that we can do \n",
    "                            # final evaluation on it\n",
    "modelGo.load_weights(filepath)\n",
    "modelGo.compile(loss='categorical_crossentropy', \n",
    "                optimizer=optmz, \n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                            # Make classification on the test dataset\n",
    "predicts    = modelGo.predict(tsDat)\n",
    "\n",
    "\n",
    "                            # Prepare the classification output\n",
    "                            # for the classification report\n",
    "predout     = np.argmax(predicts,axis=1)\n",
    "testout     = np.argmax(tsLbl,axis=1)\n",
    "labelname   = ['airplane',\n",
    "               'automobile',\n",
    "               'bird',\n",
    "               'cat',\n",
    "               'deer',\n",
    "               'dog',\n",
    "               'frog',\n",
    "               'horse',\n",
    "               'ship',\n",
    "               'truck']\n",
    "                                            # the labels for the classfication report\n",
    "\n",
    "\n",
    "testScores  = metrics.accuracy_score(testout,predout)\n",
    "confusion   = metrics.confusion_matrix(testout,predout)\n",
    "\n",
    "\n",
    "print(\"Best accuracy (on testing dataset): %.2f%%\" % (testScores*100))\n",
    "print(metrics.classification_report(testout,predout,target_names=labelname,digits=4))\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "records     = pd.read_csv(modelname +'.csv')\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "plt.plot(records['val_loss'])\n",
    "plt.plot(records['loss'])\n",
    "plt.yticks([0,0.20,0.40,0.60,0.80,1.00])\n",
    "plt.title('Loss value',fontsize=12)\n",
    "\n",
    "ax          = plt.gca()\n",
    "ax.set_xticklabels([])\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(records['val_acc'])\n",
    "plt.plot(records['acc'])\n",
    "plt.yticks([0.6,0.7,0.8,0.9,1.0])\n",
    "plt.title('Accuracy',fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.utils import plot_model\n",
    "#\n",
    "plot_model(model, \n",
    "          to_file=modelname+'_ResNet_model.pdf', \n",
    "          show_shapes=True, \n",
    "          show_layer_names=False,\n",
    "          rankdir='TB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
